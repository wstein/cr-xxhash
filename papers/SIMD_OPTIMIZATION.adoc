= SIMD Optimization Strategy for Crystal xxHash

== Session 7: LLVM Auto-Vectorization Foundation

*Status*: ✅ Phase 1 Complete — Stack allocation and inlining optimizations in place for LLVM auto-vectorization

Date: February 8, 2026

== Executive Summary

Rather than implementing platform-specific intrinsics outright, we target LLVM auto-vectorization as the primary SIMD strategy. This approach keeps a single, maintainable Crystal code path while enabling LLVM to generate optimized vector instructions (NEON, AVX2, SSE2) where applicable.

== Session 7 Optimizations (What We Did)

* Replaced heap-allocated `Array(UInt64)` with stack-allocated `uninitialized UInt64[8]` in hot accumulator paths to provide LLVM with fixed-size contiguous working sets.
* Added `@[AlwaysInline]` to critical accumulation functions so the compiler sees full loop bodies and can analyse them for vectorization.
* Refactored indexing to use explicit indices/pointers (e.g., `Pointer(UInt64)` parameters) to avoid value-type copying and to make alias analysis easier.
* Verified behavior with full unit test suite and benchmark harness.

== Why This Matters

* Fixed-size stack arrays remove bounds checks and indirection, enabling LLVM's loop vectorizer to operate on the accumulator lanes.
* Inlining exposes the inner loop body where independent lane updates become apparent to the compiler, resulting in SIMD-friendly IR.

== Implementation Notes

* Files touched (examples): `src/xxh/xxh3.cr` (long paths and State classes), `src/xxh/xxh64.cr`, `src/xxh/xxh32.cr`.
* Key patterns introduced:
** `uninitialized UInt64[8]`
** `@[AlwaysInline]` on scalar kernels
** Function parameters as `Pointer(UInt64)` for accumulator references

== Verification & Benchmarks

* Use `crystal build ... --emit llvm-ir` and inspect IR for `<n x i64>` vector types.
* Use `objdump -d bin/xxh3sum | grep -E "vmovdqu|vpadduq|neon"` to spot vector instructions.
* Benchmarks after Session 7 show large-input gains; specific results collected in the project benchmark harness.

== Next Steps

1. Profile with `perf`/sampling tools to confirm vector instruction emission and measure CPI improvements.
2. If auto-vectorization underperforms targets, implement manual intrinsics or FFI helper paths for AVX2/NEON.
3. Add optional `--simd=avx2|neon` CLI flags that map to tested, platform-specific paths.

== Full Session Report

For full technical detail and the implementation checklist, see `SIMD_OPTIMIZATION_STRATEGY.md` in the repository history (now superseded by this document).

:toc:
