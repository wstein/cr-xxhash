== Native xxHash Implementation Strategy

This section documents the design and implementation approach for porting the xxHash library from C99 to idiomatic yet high-performance Crystal. The goal is to achieve >90% of native C throughput while maintaining bit-identical outputs and zero external dependencies.

=== 12.1 Architecture Overview

The native implementation replaces FFI bindings with pure Crystal code organized in a composable, dispatch-driven architecture:

[source]
----
src/
├── xxh/
│   ├── primitives.cr          # Low-level bit ops, rotations, endian reads
│   ├── common.cr              # Constants, primes, buffer management
│   ├── dispatch.cr            # CPU detection, SIMD routing
│   ├── xxh32.cr               # XXH32 scalar implementation
│   ├── xxh64.cr               # XXH64 scalar implementation
│   ├── xxh3/
│   │   ├── scalar.cr          # XXH3 scalar (fallback)
│   │   ├── neon.cr            # ARM NEON (Apple Silicon)
│   │   ├── avx2.cr            # x86 AVX2
│   │   ├── avx512.cr          # x86 AVX-512 (future)
│   │   └── dispatch.cr        # Runtime CPU selection
│   └── state.cr               # Streaming state structs
└── cli/
    └── hasher.cr              # Updated to use native impl
----

=== 12.2 Memory & Buffer Strategy

**Goal**: Zero-copy, reusable buffers with predictable allocation patterns.

.Static Buffer Allocation
[source,crystal]
----
module XXH
  @@scratch_buffer : Bytes?
  @@stripe_buffer : Bytes?
  @@secret_buffer : Bytes?
  @@acc_buffer : Pointer(Void)?

  # Lazy initialization (once per process)
  def self.scratch_buffer : Bytes
    @@scratch_buffer ||= Bytes.new(256)
  end

  # SIMD-aligned allocation (64-byte cache lines)
  def self.aligned_alloc(size : Int32, alignment : Int32 = 64) : Pointer(Void)
    LibC.posix_memalign(out ptr, alignment.to_size, size.to_size)
    ptr
  end
end
----

**Key Properties**:
- Single allocation per buffer type (reused forever)
- No per-hash allocations after initialization
- Cache-line aligned (64 bytes) for SIMD
- Fallback to standard malloc on systems without posix_memalign

=== 12.3 Primitives & Low-Level Operations

**Goal**: Extract bit-level operations into @[AlwaysInline] primitives for cross-boundary optimization.

.Rotate Operations (LLVM IR generation)
[source,crystal]
----
module XXH::Primitives
  @[AlwaysInline]
  def self.rotl32(x : UInt32, r : Int32) : UInt32
    (x << r) | (x >> (32 - r))
  end

  @[AlwaysInline]
  def self.rotl64(x : UInt64, r : Int32) : UInt64
    (x << r) | (x >> (64 - r))
  end

  # Endian-aware reads with pointer arithmetic
  @[AlwaysInline]
  def self.read_u32_le(ptr : Pointer(UInt8)) : UInt32
    ptr.as(Pointer(UInt32)).value
  end

  @[AlwaysInline]
  def self.read_u64_le(ptr : Pointer(UInt8)) : UInt64
    ptr.as(Pointer(UInt64)).value
  end

  # Conditional byteswap for big-endian systems
  @[AlwaysInline]
  def self.bswap64(x : UInt64) : UInt64
    {% if flag?(:little_endian) %}
      x
    {% else %}
      Intrinsics.bswap64(x)
    {% end %}
  end
end
----

=== 12.4 CPU Detection & Dispatch

**Goal**: Single runtime detection with cached results. Route calls to optimal SIMD implementation.

.Dispatch Module
[source,crystal]
----
module XXH::Dispatch
  enum CPUFeature
    SSE2    = 1 << 0
    AVX2    = 1 << 1
    AVX512  = 1 << 2
    NEON    = 1 << 3
  end

  @@cpu_features : CPUFeature = detect_cpu_features

  def self.detect_cpu_features : CPUFeature
    features = CPUFeature::None

    {% if flag?(:x86_64) %}
      # CPUID instruction: detect AVX2, AVX-512
      # eax=1: SSE4.1 (ecx bit 19), AVX (ecx bit 28)
      # eax=7: AVX2 (ebx bit 5), AVX-512F (ebx bit 16)
    {% elsif flag?(:aarch64) %}
      # NEON always present on AArch64 v8.0+
      features |= CPUFeature::NEON
    {% end %}

    features
  end

  def self.hash_xxh3(input : Bytes, seed : UInt64) : UInt64
    case @@cpu_features
    when .avx512?
      XXH3::AVX512.hash(input, seed)
    when .avx2?
      XXH3::AVX2.hash(input, seed)
    when .neon?
      XXH3::NEON.hash(input, seed)
    else
      XXH3::Scalar.hash(input, seed)
    end
  end
end
----

=== 12.5 SIMD Implementation Strategy

==== 12.5.1 ARM NEON (Apple Silicon M1/M2/M3/M4)

**Target**: 50+ GB/s throughput using 128-bit vectors.

.NEON Vector Operations (Conditional Compilation)
[source,crystal]
----
{% if flag?(:aarch64) %}
module XXH::NEON
  @[AlwaysInline]
  def self.vld1q_u64(ptr : Pointer(UInt8)) : UInt64x2
    ptr.as(Pointer(UInt64x2)).value
  end

  @[AlwaysInline]
  def self.veorq(a : UInt64x2, b : UInt64x2) : UInt64x2
    a ^ b
  end

  # Multiply lower 32-bit lanes and widen result
  @[AlwaysInline]
  def self.vmulq_u32(a : UInt32x4, b : UInt32x4) : UInt64x2
    Intrinsics.aarch64_neon_umull(a, b)
  end
end
{% end %}
----

==== 12.5.2 x86 AVX2 (Intel/AMD)

**Target**: 30+ GB/s throughput using 256-bit vectors.

.AVX2 Vector Operations (Conditional Compilation)
[source,crystal]
----
{% if flag?(:x86_64) %}
module XXH::AVX2
  @[AlwaysInline]
  def self.load256(ptr : Pointer(UInt8)) : UInt256
    ptr.as(Pointer(UInt256)).value
  end

  @[AlwaysInline]
  def self.mul_epu32(a : UInt256, b : UInt256) : UInt256
    Intrinsics.x86_avx2_pmulu_epi32(a, b)
  end

  @[AlwaysInline]
  def self.permute(v : UInt256, mask : Int32) : UInt256
    Intrinsics.x86_avx2_permute2f128(v, v, mask)
  end
end
{% end %}
----

=== 12.6 Implementation Phases (Priority Order)

[cols="1,2,3,1"]
|===
| Phase | Component | Effort | Validation

| P1
| XXH32 scalar + dispatch skeleton
| 6h
| 100+ test vectors vs FFI

| P1
| XXH64 scalar
| 4h
| 100+ test vectors vs FFI

| P2
| XXH3 scalar (baseline)
| 8h
| Streaming API validation

| P2
| ARM NEON (XXH3 accumulation)
| 6h
| Benchmark on M1/M4, vs FFI

| P3
| x86 AVX2 (XXH3 accumulation)
| 8h
| Benchmark on x86-64 Xeon

| P3
| Parallel file processing (fibers)
| 4h
| Multi-file throughput test

| P4
| Memory mapping (large files > 1MB)
| 3h
| I/O benchmark test

| P5
| x86 AVX-512 (future)
| 10h
| High-end hardware validation
|===

**Total Estimate**: 40–60 hours for P1–P3, targeting 90%+ C parity.

=== 12.7 Streaming State API

**Goal**: Opaque state struct matching C API, supporting incremental hashing.

.State Definition
[source,crystal]
----
module XXH
  struct XXH32_State
    @acc_v1 : UInt32
    @acc_v2 : UInt32
    @acc_v3 : UInt32
    @acc_v4 : UInt32
    @total_len : UInt64
    @buffer : StaticArray(UInt8, 16)
    @buffer_size : UInt32

    def initialize(seed : UInt32 = 0)
      reset(seed)
    end

    def reset(seed : UInt32)
      @acc_v1 = seed &+ Primitives::PRIME32_1 + Primitives::PRIME32_2
      @acc_v2 = seed &+ Primitives::PRIME32_2
      @acc_v3 = seed
      @acc_v4 = seed &- Primitives::PRIME32_1
      @total_len = 0_u64
      @buffer_size = 0_u32
    end

    def update(input : Bytes) : Nil
      len = input.size
      @total_len = @total_len &+ len

      if @buffer_size + len < 16
        # Fill internal buffer
        input.copy_to(@buffer.to_unsafe + @buffer_size, len)
        @buffer_size += len.to_u32
      else
        # Process buffer + input
        process_buffer
        process_stripes(input)
      end
    end

    def digest : UInt32
      finalize
    end
  end
end
----

=== 12.8 Performance Targets

[cols="1,2,2,1"]
|===
| Algorithm | SIMD Path | Target Throughput | C Reference

| XXH32
| Scalar
| 2 GB/s
| 3 GB/s

| XXH64
| Scalar
| 4 GB/s
| 6 GB/s

| XXH3
| NEON (M4)
| 45–50 GB/s
| 52 GB/s

| XXH3
| AVX2 (x86-64)
| 28–32 GB/s
| 35 GB/s

| XXH3
| AVX-512 (x86)
| 60+ GB/s
| 70+ GB/s
|===

**Success Criterion**: >90% of C throughput in each category.

=== 12.9 Integration with Existing FFI

**Transition Strategy**:

1. **Phase 1**: Implement native code alongside existing FFI bindings
2. **Phase 2**: Add native-vs-FFI verification tests
3. **Phase 3**: Switch default to native, keep FFI as fallback
4. **Phase 4**: Remove FFI once P1–P3 complete and tested

.Feature Toggle Pattern
[source,crystal]
----
module XXH
  {% if flag?(:native) %}
    def self.hash64(input : Bytes, seed : UInt64 = 0) : UInt64
      Dispatch.hash_xxh64(input, seed)
    end
  {% else %}
    def self.hash64(input : Bytes, seed : UInt64 = 0) : UInt64
      FFI.hash64(input.to_unsafe, input.size, seed)
    end
  {% end %}
end
----

Compile with `--define native` to use native implementation, fallback to FFI otherwise.

=== 12.10 Testing & Validation

.Test Categories
[source]
----
spec/
├── xxh32_spec.cr              # 50+ vectors vs reference
├── xxh64_spec.cr              # 50+ vectors vs reference
├── xxh3_spec.cr               # 100+ vectors, streaming validation
├── neon_spec.cr               # Apple Silicon SIMD validation
├── avx2_spec.cr               # x86-64 SIMD validation
├── dispatch_spec.cr           # CPU detection verification
└── integration/
    ├── native_vs_ffi.cr       # Bit-identical output comparison
    └── performance.cr         # Throughput benchmarking
----

**Validation Process**:

1. For each new SIMD path, verify against FFI reference on real hardware
2. Run full test suite with `-Dnative` flag enabled
3. Benchmark and document throughput vs C implementation
4. Ensure zero regressions in CLI tool

=== 12.11 Documentation & Contributor Guide

Updates to papers/CONTRIBUTING.adoc will include:

* **Intrinsic Patterns**: Examples of how to write LLVM intrinsic calls
* **CPU Detection**: Guide for adding new ISA features (e.g., AVX-512)
* **Benchmarking**: Instructions for running performance tests
* **Testing Native Code**: How to validate native vs FFI parity

Example contributor task:

[source,asciidoc]
----
=== Adding AVX-512 Support

1. Create `src/xxh/avx512.cr` with 512-bit vector primitives
2. Add `AVX512` variant to `CPUFeature` enum in dispatch.cr
3. Update CPUID detection for AVX-512F (eax=7, ebx bit 16)
4. Implement `XXH3::AVX512.hash()` and `.update()` methods
5. Add tests in `spec/avx512_spec.cr`
6. Benchmark and document results in `tmp/NATIVE_IMPLEMENTATION_ROADMAP.md`
----

=== 12.12 Success Metrics

Upon completion of phases P1–P3, the native implementation will be considered successful if:

* **Correctness**: 100% of test vectors pass for XXH32, XXH64, XXH3
* **Performance**: ≥90% throughput of C reference on all major platforms
* **Integration**: CLI tool automatically uses native with zero API changes
* **Documentation**: MIGRATION.adoc includes all design decisions and rationale
* **Maintainability**: Code uses idiomatic Crystal patterns with unsafe blocks clearly documented

---

*Next Steps: Begin with Phase P1 (XXH32 scalar + dispatch) to establish foundation for SIMD implementations.*
