== 11 Native xxHash Implementation Strategy

This section documents the design and implementation approach for porting the xxHash library from C99 to idiomatic yet high-performance Crystal. The goal is to achieve >90% of native C throughput while maintaining bit-identical outputs and zero external dependencies.

=== 11.1 Architecture Overview

The native implementation replaces FFI bindings with pure Crystal code organized in a composable, dispatch-driven architecture:

[source]
----
src/
â”œâ”€â”€ xxh/
â”‚   â”œâ”€â”€ primitives.cr          # Low-level bit ops, rotations, endian reads
â”‚   â”œâ”€â”€ common.cr              # Constants, primes, buffer management
â”‚   â”œâ”€â”€ dispatch.cr            # CPU detection, SIMD routing
â”‚   â”œâ”€â”€ xxh32.cr               # XXH32 scalar implementation
â”‚   â”œâ”€â”€ xxh64.cr               # XXH64 scalar implementation
â”‚   â”œâ”€â”€ xxh3/
â”‚   â”‚   â”œâ”€â”€ scalar.cr          # XXH3 scalar (fallback)
â”‚   â”‚   â”œâ”€â”€ neon.cr            # ARM NEON (Apple Silicon)
â”‚   â”‚   â”œâ”€â”€ avx2.cr            # x86 AVX2
â”‚   â”‚   â”œâ”€â”€ avx512.cr          # x86 AVX-512 (future)
â”‚   â”‚   â””â”€â”€ dispatch.cr        # Runtime CPU selection
â”‚   â””â”€â”€ state.cr               # Streaming state structs
â””â”€â”€ cli/
    â””â”€â”€ hasher.cr              # Updated to use native impl
----

=== 11.2 Memory & Buffer Strategy

**Goal**: Zero-copy, reusable buffers with predictable allocation patterns.

.Static Buffer Allocation
[source,crystal]
----
module XXH
  @@scratch_buffer : Bytes?
  @@stripe_buffer : Bytes?
  @@secret_buffer : Bytes?
  @@acc_buffer : Pointer(Void)?

  # Lazy initialization (once per process)
  def self.scratch_buffer : Bytes
    @@scratch_buffer ||= Bytes.new(256)
  end

  # SIMD-aligned allocation (64-byte cache lines)
  def self.aligned_alloc(size : Int32, alignment : Int32 = 64) : Pointer(Void)
    LibC.posix_memalign(out ptr, alignment.to_size, size.to_size)
    ptr
  end
end
----

**Key Properties**:
- Single allocation per buffer type (reused forever)
- No per-hash allocations after initialization
- Cache-line aligned (64 bytes) for SIMD
- Fallback to standard malloc on systems without posix_memalign

=== 11.3 Primitives & Low-Level Operations

**Goal**: Extract bit-level operations into @[AlwaysInline] primitives for cross-boundary optimization.

.Rotate Operations (LLVM IR generation)
[source,crystal]
----
module XXH::Primitives
  @[AlwaysInline]
  def self.rotl32(x : UInt32, r : Int32) : UInt32
    (x << r) | (x >> (32 - r))
  end

  @[AlwaysInline]
  def self.rotl64(x : UInt64, r : Int32) : UInt64
    (x << r) | (x >> (64 - r))
  end

  # Endian-aware reads with pointer arithmetic
  @[AlwaysInline]
  def self.read_u32_le(ptr : Pointer(UInt8)) : UInt32
    ptr.as(Pointer(UInt32)).value
  end

  @[AlwaysInline]
  def self.read_u64_le(ptr : Pointer(UInt8)) : UInt64
    ptr.as(Pointer(UInt64)).value
  end

  # Conditional byteswap for big-endian systems
  @[AlwaysInline]
  def self.bswap64(x : UInt64) : UInt64
    {% if flag?(:little_endian) %}
      x
    {% else %}
      Intrinsics.bswap64(x)
    {% end %}
  end
end
----

=== 11.4 CPU Detection & Dispatch

**Goal**: Single runtime detection with cached results. Route calls to optimal SIMD implementation.

.Dispatch Module
[source,crystal]
----
module XXH::Dispatch
  enum CPUFeature
    SSE2    = 1 << 0
    AVX2    = 1 << 1
    AVX512  = 1 << 2
    NEON    = 1 << 3
  end

  @@cpu_features : CPUFeature = detect_cpu_features

  def self.detect_cpu_features : CPUFeature
    features = CPUFeature::None

    {% if flag?(:x86_64) %}
      # CPUID instruction: detect AVX2, AVX-512
      # eax=1: SSE4.1 (ecx bit 19), AVX (ecx bit 28)
      # eax=7: AVX2 (ebx bit 5), AVX-512F (ebx bit 16)
    {% elsif flag?(:aarch64) %}
      # NEON always present on AArch64 v8.0+
      features |= CPUFeature::NEON
    {% end %}

    features
  end

  def self.hash_xxh3(input : Bytes, seed : UInt64) : UInt64
    case @@cpu_features
    when .avx512?
      XXH3::AVX512.hash(input, seed)
    when .avx2?
      XXH3::AVX2.hash(input, seed)
    when .neon?
      XXH3::NEON.hash(input, seed)
    else
      XXH3::Scalar.hash(input, seed)
    end
  end
end
----

=== 11.5 SIMD Implementation Strategy

// Session 7 summary: advanced vectorization readiness and strategy
include::migration/12_simd_optimization.adoc[]


==== 11.5.1 ARM NEON (Apple Silicon M1/M2/M3/M4)

**Target**: 50+ GB/s throughput using 128-bit vectors.

.NEON Vector Operations (Conditional Compilation)
[source,crystal]
----
{% if flag?(:aarch64) %}
module XXH::NEON
  @[AlwaysInline]
  def self.vld1q_u64(ptr : Pointer(UInt8)) : UInt64x2
    ptr.as(Pointer(UInt64x2)).value
  end

  @[AlwaysInline]
  def self.veorq(a : UInt64x2, b : UInt64x2) : UInt64x2
    a ^ b
  end

  # Multiply lower 32-bit lanes and widen result
  @[AlwaysInline]
  def self.vmulq_u32(a : UInt32x4, b : UInt32x4) : UInt64x2
    Intrinsics.aarch64_neon_umull(a, b)
  end
end
{% end %}
----

==== 12.5.2 x86 AVX2 (Intel/AMD)

**Target**: 30+ GB/s throughput using 256-bit vectors.

.AVX2 Vector Operations (Conditional Compilation)
[source,crystal]
----
{% if flag?(:x86_64) %}
module XXH::AVX2
  @[AlwaysInline]
  def self.load256(ptr : Pointer(UInt8)) : UInt256
    ptr.as(Pointer(UInt256)).value
  end

  @[AlwaysInline]
  def self.mul_epu32(a : UInt256, b : UInt256) : UInt256
    Intrinsics.x86_avx2_pmulu_epi32(a, b)
  end

  @[AlwaysInline]
  def self.permute(v : UInt256, mask : Int32) : UInt256
    Intrinsics.x86_avx2_permute2f128(v, v, mask)
  end
end
{% end %}
----

=== 12.6 Implementation Phases (Final Status)

[cols="1,2,3,1"]
|===
| Phase | Component | Effort | Validation

| P1
| XXH32 scalar + dispatch skeleton
| 6h
| âœ… Complete

| P1
| XXH64 scalar
| 4h
| âœ… Complete

| P2
| XXH3 scalar (baseline)
| 8h
| âœ… Complete

| P2
| LLVM Auto-Vectorization (XXH3)
| 10h
| âœ… Reached 30 GB/s

| P3
| Streaming API Consolidation (DRY)
| 12h
| âœ… Complete

| P4
| Parallel file processing (fibers)
| 4h
| ðŸŸ¦ Out of scope

| P5
| x86 AVX-512 / Platform Intrinsics
| 10h
| ðŸŸ¦ Canceled (Auto-vec preferred)
|===

**Conclusion**: The implementation achieved its primary goals by leveraging LLVM's auto-vectorization capabilities. The shift from manual assembly to compiler-assisted optimization resulted in 30 GB/s throughput for XXH3, providing a high-performance, cross-platform solution with a single code base.

==== 12.6.1 XXH3 native implementation details

The native XXH3 implementation has been split into a **64-bit** and **128-bit** component to keep the public APIs unambiguous and the internals modular:

- **Files & structure**:
  - `src/xxh/xxh3/xxh3_64.cr` â€” New 64-bit XXH3 implementation (one-shot: `hash64`, `hash64_with_seed`, long input handling and `finalize_long_64b`).
  - `src/xxh/xxh3/xxh3_128.cr` â€” 128-bit implementation (one-shot: `hash128`, `hash128_with_seed`, streaming finalization into `Hash128`).

- **Public API**:
  - `XXH::XXH3.hash(input)` and `XXH::XXH3.hash_with_seed(input, seed)` refer to the **64-bit** variant and return `UInt64`.
  - `XXH::XXH3.hash128(input)` and `XXH::XXH3.hash128_with_seed(input, seed)` return a `Hash128` struct.
  - Rationale: avoiding overloads with differing return types keeps method dispatch explicit and prevents ambiguity in streaming state digests.

- **Modularity & helpers**:
  - Common helpers and constants (e.g. `mix16b`, `mul128_fold64`, `xx3_avalanche`, `merge_accs`, `INIT_ACC`) live in `src/xxh/xxh3/xxh3_base.cr` and `src/xxh/xxh3/xxh3_types.cr`.
  - Both implementations explicitly `require` these modules and call the helpers with qualified names (`XXH::XXH3.mix16b`, `XXH::XXH3.mul128_fold64`, etc.) to make origin and intent unambiguous.

- **Validation**:
  - The implementation was tested against the C99 reference outputs across a wide range of inputs (0..4096+ bytes), seeds, and boundary cases. All XXH3 native specs pass locally (110 examples, 0 failures).

This approach preserves bit-identical behavior with the vendor reference while keeping the Crystal code clear, testable, and maintainable.

=== 12.7 Streaming State API

**Goal**: Opaque state struct matching C API, supporting incremental hashing.

.State Definition
[source,crystal]
----
module XXH
  struct XXH32_State
    @acc_v1 : UInt32
    @acc_v2 : UInt32
    @acc_v3 : UInt32
    @acc_v4 : UInt32
    @total_len : UInt64
    @buffer : StaticArray(UInt8, 16)
    @buffer_size : UInt32

    def initialize(seed : UInt32 = 0)
      reset(seed)
    end

    def reset(seed : UInt32)
      @acc_v1 = seed &+ Primitives::PRIME32_1 + Primitives::PRIME32_2
      @acc_v2 = seed &+ Primitives::PRIME32_2
      @acc_v3 = seed
      @acc_v4 = seed &- Primitives::PRIME32_1
      @total_len = 0_u64
      @buffer_size = 0_u32
    end

    def update(input : Bytes) : Nil
      len = input.size
      @total_len = @total_len &+ len

      if @buffer_size + len < 16
        # Fill internal buffer
        input.copy_to(@buffer.to_unsafe + @buffer_size, len)
        @buffer_size += len.to_u32
      else
        # Process buffer + input
        process_buffer
        process_stripes(input)
      end
    end

    def digest : UInt32
      finalize
    end
  end
end
----

=== 12.8 Performance Targets

[cols="1,2,2,1"]
|===
| Algorithm | SIMD Path | Target Throughput | C Reference

| XXH32
| Scalar
| 2 GB/s
| 3 GB/s

| XXH64
| Scalar
| 4 GB/s
| 6 GB/s

| XXH3
| NEON (M4)
| 45â€“50 GB/s
| 52 GB/s

| XXH3
| AVX2 (x86-64)
| 28â€“32 GB/s
| 35 GB/s

| XXH3
| AVX-512 (x86)
| 60+ GB/s
| 70+ GB/s
|===

**Success Criterion**: >90% of C throughput in each category.

=== 12.9 Integration with Existing FFI

**Transition Strategy**:

1. **Phase 1**: Implement native code alongside existing FFI bindings
2. **Phase 2**: Add native-vs-FFI verification tests
3. **Phase 3**: Switch default to native, keep FFI as fallback
4. **Phase 4**: Remove FFI once P1â€“P3 complete and tested

.Feature Toggle Pattern
[source,crystal]
----
module XXH
  {% if flag?(:native) %}
    def self.hash64(input : Bytes, seed : UInt64 = 0) : UInt64
      Dispatch.hash_xxh64(input, seed)
    end
  {% else %}
    def self.hash64(input : Bytes, seed : UInt64 = 0) : UInt64
      FFI.hash64(input.to_unsafe, input.size, seed)
    end
  {% end %}
end
----

Compile with `--define native` to use native implementation, fallback to FFI otherwise.

=== 12.10 Testing & Validation

.Test Categories
[source]
----
spec/
â”œâ”€â”€ xxh32_spec.cr              # 50+ vectors vs reference
â”œâ”€â”€ xxh64_spec.cr              # 50+ vectors vs reference
â”œâ”€â”€ xxh3_spec.cr               # 100+ vectors, streaming validation
â”œâ”€â”€ neon_spec.cr               # Apple Silicon SIMD validation
â”œâ”€â”€ avx2_spec.cr               # x86-64 SIMD validation
â”œâ”€â”€ dispatch_spec.cr           # CPU detection verification
â””â”€â”€ integration/
    â”œâ”€â”€ native_vs_ffi.cr       # Bit-identical output comparison
    â””â”€â”€ performance.cr         # Throughput benchmarking
----

**Validation Process**:

1. For each new SIMD path, verify against FFI reference on real hardware
2. Run full test suite with `-Dnative` flag enabled
3. Benchmark and document throughput vs C implementation
4. Ensure zero regressions in CLI tool

=== 12.11 Documentation & Contributor Guide

Updates to papers/CONTRIBUTING.adoc will include:

* **Intrinsic Patterns**: Examples of how to write LLVM intrinsic calls
* **CPU Detection**: Guide for adding new ISA features (e.g., AVX-512)
* **Benchmarking**: Instructions for running performance tests
* **Testing Native Code**: How to validate native vs FFI parity

Example contributor task:

[source,asciidoc]
----
=== Adding AVX-512 Support

1. Create `src/xxh/avx512.cr` with 512-bit vector primitives
2. Add `AVX512` variant to `CPUFeature` enum in dispatch.cr
3. Update CPUID detection for AVX-512F (eax=7, ebx bit 16)
4. Implement `XXH3::AVX512.hash()` and `.update()` methods
5. Add tests in `spec/avx512_spec.cr`
6. Benchmark and document results in `tmp/NATIVE_IMPLEMENTATION_ROADMAP.md`
----

=== 12.12 Success Metrics

Upon completion of phases P1â€“P3, the native implementation will be considered successful if:

* **Correctness**: 100% of test vectors pass for XXH32, XXH64, XXH3
* **Performance**: â‰¥90% throughput of C reference on all major platforms
* **Integration**: CLI tool automatically uses native with zero API changes
* **Documentation**: MIGRATION.adoc includes all design decisions and rationale
* **Maintainability**: Code uses idiomatic Crystal patterns with unsafe blocks clearly documented

---

*Next Steps: Begin with Phase P1 (XXH32 scalar + dispatch) to establish foundation for SIMD implementations.*
